{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1.What is a Decision Tree, and how does it work in the context of\n",
        "classification?**\n",
        "\n",
        "* A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. In the context of classification, it is one of the most intuitive and easy-to-understand algorithms because it mimics human decision-making.\n",
        "\n",
        "* A Decision Tree is a flowchart-like structure:\n",
        "* Each internal node represents a decision based on a feature (attribute).\n",
        "* Each branch represents the outcome of that decision.\n",
        "* Each leaf node represents a class label (the predicted outcome).\n",
        "* The model splits the dataset into subsets based on the most significant features, recursively, until it reaches a stopping condition (like max depth, minimum samples, or pure class labels).\n",
        "* Root Node Selection\n",
        "  * The algorithm chooses the best feature to split the dataset (based on metrics like Gini Impurity, Entropy/Information Gain, or Chi-square).\n",
        "* Splitting\n",
        "    * The dataset is split into subsets based on the chosen feature’s values.\n",
        "    * For example, if the feature is \"Age < 30\", the tree creates two branches: one for \"Yes\" and one for \"No\".\n",
        "* Recursive Partitioning\n",
        "      * The algorithm continues to split each subset into further nodes using the same process.\n",
        "      * This creates a hierarchy of decisions that narrows down possibilities.\n",
        "* Leaf Nodes (Classification Output)\n",
        "    * Splitting stops when:\n",
        "        * All samples in a node belong to the same class (pure node).\n",
        "        * Or maximum tree depth/minimum samples is reached.\n",
        "        * The leaf node assigns a class label (majority class of samples in that node).\n",
        "* Example\n",
        "    * Suppose we want to classify whether a person will buy a computer:\n",
        "    * Features: Age, Income, Student Status, Credit Rating.\n",
        "\n",
        "* Advantages\n",
        "    * Easy to interpret and visualize.\n",
        "    * No need for feature scaling (works with categorical and numerical data).\n",
        "    * Can capture nonlinear relationships.\n",
        "\n",
        "* Disadvantages\n",
        "   * Prone to overfitting if not pruned.\n",
        "   * Can be unstable (small changes in data can lead to a different tree).\n",
        "   * Biased toward features with more levels.\n",
        "\n",
        "**2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "* 1. Gini Impurity\n",
        "* Definition: Gini impurity measures the probability of incorrectly classifying a randomly chosen sample if it was labeled according to the class distribution in that node.\n",
        "\n",
        "Formula:\n",
        " * Gini=1−i=1∑Cpi2\n",
        " * Where: C = number of classes pi = proportion of samples belonging to class\n",
        "     * i in the node\n",
        "     * Range:\n",
        "       * Minimum = 0 (pure node: all samples in one class)\n",
        "       * Maximum = 1 - \\frac{1}{C} (highest impurity, evenly distributed classes)\n",
        " * Example: If a node has 10 samples → 4 in Class A, 6 in Class B:\n",
        "      * pA=0.4,pB=0.6 Gini=1−(0.42+0.62)=1−(0.16+0.36)=0.48.\n",
        "\n",
        "2. Entropy (Information Gain)\n",
        "* Definition: Entropy measures the \"disorder\" or \"uncertainty\" in a dataset. A pure node has entropy = 0, while mixed classes have higher entropy.\n",
        " * Formula: Entropy=−i=1∑Cpi log2 (pi)\n",
        " * Range:\n",
        "   * Minimum = 0 (pure node: all samples in one class)\n",
        "   * Maximum = \\log_2(C) (when classes are equally likely)\n",
        "   * Example (same node as before: 40% A, 60% B):\n",
        "        * Entropy =−[(0.4×−1.32)+(0.6×−0.74)]=0.97\n",
        "* How they impact Decision Tree Splits\n",
        " * At each split, the Decision Tree algorithm:\n",
        "    * Calculates impurity before splitting (parent node).\n",
        "    * Calculates impurity after splitting (child nodes).\n",
        "    * Chooses the feature and threshold that maximizes the impurity reduction.\n",
        " * Impurity Reduction (Information Gain):\n",
        "       * Gain=Impurity parent − k=1∑mnk\n",
        "\t     * Impurity child k\n",
        "\n",
        "\t​* Where:\n",
        "  * n = total samples in parent\n",
        "  * nk = samples in child\n",
        "  * k The split with the highest gain is chosen.\n",
        "\n",
        "**3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.**\n",
        "\n",
        "* Pruning in Decision Trees\n",
        "   * Decision Trees, if left unregulated, can grow very deep, perfectly fitting the training data.\n",
        "   * This often leads to overfitting (great accuracy on training data, poor on test data).\n",
        "   * Pruning reduces tree complexity by limiting growth (Pre-Pruning) or trimming after full growth (Post-Pruning).\n",
        "\n",
        "**1. Pre-Pruning (Early Stopping)**\n",
        " * The tree stops growing early, before it becomes too complex.\n",
        " * Done by setting constraints such as:\n",
        "    * Maximum depth (max_depth)\n",
        "    * Minimum samples required to split a node (min_samples_split)\n",
        "    * Minimum samples per leaf (min_samples_leaf)\n",
        "    * Minimum impurity decrease (min_impurity_decrease)\n",
        "* Practical Advantage:\n",
        "   * Saves computation time and memory, since the tree is never fully grown.\n",
        "   * Useful when working with large datasets where training speed matters.\n",
        "\n",
        "**2. Post-Pruning (Pruning After Full Growth)**\n",
        "* First, the tree is allowed to grow fully.\n",
        "* Then, branches that contribute little to predictive power are cut back.\n",
        "* Techniques include:\n",
        "  * Cost Complexity Pruning (CCP) → removes branches with least importance by balancing accuracy vs complexity.\n",
        "  * Reduced Error Pruning → removes nodes if accuracy on validation data does not decrease.\n",
        "* Practical Advantage:\n",
        "   * Often yields better generalization, since the pruning decision is based on the full grown tree and validation performance.\n",
        "  * Useful when accuracy is more important than speed.\n",
        "\n",
        "**4.What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?**\n",
        "\n",
        "* Information Gain (IG) measures the reduction in impurity (uncertainty) achieved by splitting a dataset based on a particular feature.\n",
        "* In decision trees (like ID3, C4.5), it is usually based on Entropy.\n",
        "* Formula\n",
        "   * IG(S,A)=Entropy(S)−v∈Values(A) ∑ Entropy(Sv)\n",
        "   * Where:\n",
        "        * S = parent dataset\n",
        "        * A = feature we split on\n",
        "        * v = possible values of feature A\n",
        "        * Sv = subset of S where A=v\n",
        "* Start with overall entropy (uncertainty) before the split.\n",
        "* Subtract the weighted average entropy of the subsets after splitting.\n",
        "* The bigger the Information Gain, the better the split.\n",
        "* Why is it important?\n",
        "    * Decision Trees need to decide which feature to split on at each step.\n",
        "    * Information Gain tells us how much a feature reduces uncertainty in the dataset.\n",
        "    * The feature with the highest IG is chosen because it produces the purest child nodes.\n",
        "* Example\n",
        "   * Suppose we want to classify whether students \"Play Tennis\" based on \"Outlook\" (Sunny, Overcast, Rainy).\n",
        "* Parent node:\n",
        "   * 9 \"Yes\", 5 \"No\" → Entropy(Parent) ≈ 0.94\n",
        "* Splitting on Outlook gives subsets:\n",
        "  * Sunny (2 Yes, 3 No) → Entropy ≈ 0.97\n",
        "  * Overcast (4 Yes, 0 No) → Entropy = 0\n",
        "  * Rainy (3 Yes, 2 No) → Entropy ≈ 0.97\n",
        "  * Weighted Entropy ≈ 0.69 IG=0.94−0.69=0.25\n",
        "* This means splitting on Outlook reduces uncertainty by 0.25 bits of information.\n",
        "\n",
        "**5.What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?**\n",
        "* Real-World Applications of Decision Trees\n",
        "* Finance & Banking\n",
        "  * Credit scoring / loan approval → Classify whether a person is likely to repay or default.\n",
        "  * Fraud detection → Identify suspicious transactions.\n",
        "* Healthcare\n",
        "  * Disease diagnosis → Classify patients based on symptoms/test results.\n",
        "  * Treatment recommendation → Decide treatment paths based on patient history.\n",
        "* Marketing & Sales\n",
        "  * Customer segmentation → Identify which customers are likely to buy a product.\n",
        "  * Churn prediction → Predict if a customer is likely to leave.\n",
        "* Manufacturing & Operations\n",
        "  * Quality control → Detect defective products in production.\n",
        "  * Supply chain optimization → Classify demand levels for better inventory planning.\n",
        "* Retail & E-commerce\n",
        "  * Recommendation systems → Classify users’ preferences for personalized ads.\n",
        "  * Pricing decisions → Decide discounts/promotions for specific customer groups.\n",
        "* Education\n",
        "  * Student performance prediction → Predict dropout risk or success probability.\n",
        "  * Adaptive learning systems → Personalize learning paths.\n",
        "* Advantages of Decision Trees\n",
        "  * Easy to interpret & visualize → Even non-technical users can understand them.\n",
        "  * Works with both categorical & numerical data.\n",
        "  * No need for feature scaling/normalization.\n",
        "  * Captures non-linear relationships → Can split data into complex decision boundaries.\n",
        "  * Fast predictions → Good for real-time classification.\n",
        "\n",
        "* Limitations of Decision Trees\n",
        "  * Overfitting → Trees can become too deep, memorizing the training data (reduced by pruning or using ensembles like Random Forest).\n",
        "  * Instability → Small changes in data can lead to a very different tree structure.\n",
        "  * Bias toward features with many levels → Categorical variables with many distinct values may dominate splits.\n",
        "  * Not always optimal → A single tree may have lower accuracy compared to ensemble methods (Random Forest, Gradient Boosted Trees).\n",
        "\n"
      ],
      "metadata": {
        "id": "Ld9_5a-fS_SO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "EK03GN5PoymW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Feature importance\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtIR7yyWpHdy",
        "outputId": "e794ca02-3444-46ec-abf7-867f00e21966"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree."
      ],
      "metadata": {
        "id": "uYAB9qWHpenl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Fully-grown Decision Tree (no max_depth limit)\n",
        "clf_full = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Decision Tree with max_depth=3\n",
        "clf_pruned = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "y_pred_pruned = clf_pruned.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# Print comparison results\n",
        "print(\"Accuracy of fully-grown tree:\", accuracy_full)\n",
        "print(\"Accuracy of max_depth=3 tree:\", accuracy_pruned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq7KauXDps08",
        "outputId": "225bd502-8aa6-4844-d98c-2d7d10c573d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of fully-grown tree: 1.0\n",
            "Accuracy of max_depth=3 tree: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "\n",
        "● Load the California Housing dataset from sklearn\n",
        "\n",
        "● Train a Decision Tree Regressor\n",
        "\n",
        "● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "FXbtvIljp0u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# Feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, reg.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_WLsCTqqFnV",
        "outputId": "99ddd9f3-3219-4b84-8bc4-beb96a715d44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.495235205629094\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "ROURsHO8qN2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree and GridSearchCV\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring=\"accuracy\")\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Set Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THjBytY2qviC",
        "outputId": "8ec0ad07-edbd-48b6-b892-e6613db2d9bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Test Set Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting."
      ],
      "metadata": {
        "id": "G3CWixp3rHYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 1) Handle missing values — steps & rationale\n",
        "   * Understand the missingness: check if data are MCAR / MAR / MNAR. This affects choice of strategy.\n",
        "   * Simple strategies (fast & robust):\n",
        "        * Numeric: median (robust to outliers) or mean.\n",
        "   * Categorical: new category like \"missing\" or most_frequent.\n",
        "   * Advanced strategies:\n",
        "      * IterativeImputer (MICE) or KNNImputer when relationships between features are important.\n",
        "      * Keep a “missingness indicator” for important variables (a boolean column) — sometimes “missing” is predictive.\n",
        "  * Avoid leakage: fit imputers only on training data (use Pipeline/ColumnTransformer).\n",
        "* 2) Encode categorical features — options & when to use them\n",
        "  * One-Hot Encoding: good for low-cardinality categories; safe for trees (uses more columns).\n",
        "  * Ordinal / Label Encoding: fast but introduces ordinal relationships (use only if category order is meaningful).\n",
        "  * Target / Mean encoding: works well with high-cardinality categories — but must be done carefully (cross-validated target encoding) to avoid leakage / overfitting.\n",
        "  * Frequency / Count encoding: useful for high-cardinality when you want compactness.\n",
        "  * For Decision Trees, you don’t need scaling; they work with integer encodings — but be mindful of implicit order if using label encoding.\n",
        "* 3) Train a Decision Tree model — best practices\n",
        "   * Always use a pipeline: preprocessing (imputer + encoder) → classifier.\n",
        "   * Use stratified train/test split for binary disease label (preserve class ratio).\n",
        "   * Consider class_weight='balanced' or resampling (SMOTE) for imbalanced disease prevalence (apply SMOTE only on training set).\n",
        "* 4) Tune hyperparameters\n",
        "  * Important hyperparameters:\n",
        "        * max_depth, min_samples_split, min_samples_leaf, max_features\n",
        "        * criterion (gini / entropy)\n",
        "        * ccp_alpha (cost complexity pruning)\n",
        "        * class_weight\n",
        "  * Tuning strategy:\n",
        "      * Use GridSearchCV or RandomizedSearchCV with StratifiedKFold.\n",
        "      * Use scoring='roc_auc' if classes are imbalanced and you care about ranking; use f1 if you want balanced precision/recall.\n",
        "  * For unbiased final evaluation, use nested CV if possible (outer CV for performance estimate, inner CV for tuning).\n",
        "\n",
        "* 5) Evaluate performance\n",
        "  * Primary metrics: ROC AUC, Precision, Recall (sensitivity), F1-score.\n",
        "  * Important in healthcare: prioritize sensitivity (recall) if missing a disease is costly; balance with acceptable false positives.\n",
        "  * Other checks: confusion matrix, PR AUC (better than ROC for very imbalanced), calibration (is predicted probability well calibrated?), decision threshold tuning.\n",
        "  * Use calibration (Platt / isotonic) if you’ll use probabilities for clinical decision thresholds.\n",
        "  * Explainability: feature_importances_, SHAP, partial dependence plots for clinician trust.\n",
        "  * Post-deployment: continuous monitoring for data drift and performance decay.\n",
        "\n",
        "* Business value (real-world benefits)\n",
        "   * Early detection & triage: flag high-risk patients for faster follow-up, reducing time to treatment.\n",
        "   * Resource optimization: prioritize diagnostic testing or specialist visits for high-probability cases.\n",
        "   * Cost savings: reduce unnecessary testing for low-risk patients, focus resources where they matter.\n",
        "   * Improved outcomes: early intervention can lead to better clinical outcomes and lower downstream costs.\n",
        "   * Actionable insights: feature importances / SHAP can suggest which symptoms/tests most strongly predict disease — aiding clinicians and care pathway design.\n",
        "* Important caveats & deployment considerations\n",
        "   * False negatives are often more costly in healthcare — choose metrics and thresholds accordingly.\n",
        "   * Bias & fairness: audit model performance across subgroups (age, sex, ethnicity).\n",
        "   * Clinical validation: models must be validated prospectively and ideally run in pilot clinical studies before production use.\n",
        "   * Privacy & compliance: follow HIPAA / local regulations for patient data.\n",
        "   * Monitoring & retraining: data distributions change — set up monitoring and periodic retraining.\n",
        "   * Explainability: clinicians require interpretable outputs — pair the model with explanations (feature importances, SHAP)."
      ],
      "metadata": {
        "id": "0kLOwsxTrbQs"
      }
    }
  ]
}